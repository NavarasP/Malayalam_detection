--- /opt/conda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/loss.py
+++ /opt/conda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/loss.py
@@ -1,158 +1,92 @@
 class CrossEntropyLoss(_WeightedLoss):
-    r"""This criterion computes the cross entropy loss between input logits
-    and target.
+    r"""This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.
 
     It is useful when training a classification problem with `C` classes.
     If provided, the optional argument :attr:`weight` should be a 1D `Tensor`
     assigning weight to each of the classes.
     This is particularly useful when you have an unbalanced training set.
 
-    The `input` is expected to contain the unnormalized logits for each class (which do `not` need
-    to be positive or sum to 1, in general).
-    `input` has to be a Tensor of size :math:`(C)` for unbatched input,
-    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1` for the
-    `K`-dimensional case. The last being useful for higher dimension inputs, such
-    as computing cross entropy loss per-pixel for 2D images.
+    The `input` is expected to contain raw, unnormalized scores for each class.
 
-    The `target` that this criterion expects should contain either:
+    `input` has to be a Tensor of size either :math:`(minibatch, C)` or
+    :math:`(minibatch, C, d_1, d_2, ..., d_K)`
+    with :math:`K \geq 1` for the `K`-dimensional case (described later).
 
-    - Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if
-      `ignore_index` is specified, this loss also accepts this class index (this index
-      may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`
-      set to ``'none'``) loss for this case can be described as:
+    This criterion expects a class index in the range :math:`[0, C-1]` as the
+    `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`
+    is specified, this criterion also accepts this class index (this index may not
+    necessarily be in the class range).
 
-      .. math::
-          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
-          l_n = - w_{y_n} \log \frac{\exp(x_{n,y_n})}{\sum_{c=1}^C \exp(x_{n,c})}
-          \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}
+    The loss can be described as:
 
-      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,
-      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as
-      :math:`d_1, ..., d_k` for the `K`-dimensional case. If
-      :attr:`reduction` is not ``'none'`` (default ``'mean'``), then
+    .. math::
+        \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
+                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)
 
-      .. math::
-          \ell(x, y) = \begin{cases}
-              \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n} \cdot \mathbb{1}\{y_n \not= \text{ignore\_index}\}} l_n, &
-               \text{if reduction} = \text{`mean';}\\
-                \sum_{n=1}^N l_n,  &
-                \text{if reduction} = \text{`sum'.}
-            \end{cases}
+    or in the case of the :attr:`weight` argument being specified:
 
-      Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`
-      on an input, followed by :class:`~torch.nn.NLLLoss`.
+    .. math::
+        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)
 
-    - Probabilities for each class; useful when labels beyond a single class per minibatch item
-      are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with
-      :attr:`reduction` set to ``'none'``) loss for this case can be described as:
+    The losses are averaged across observations for each minibatch.
 
-      .. math::
-          \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
-          l_n = - \sum_{c=1}^C w_c \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} y_{n,c}
+    Can also be used for higher dimension inputs, such as 2D images, by providing
+    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,
+    where :math:`K` is the number of dimensions, and a target of appropriate shape
+    (see below).
 
-      where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,
-      :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as
-      :math:`d_1, ..., d_k` for the `K`-dimensional case. If
-      :attr:`reduction` is not ``'none'`` (default ``'mean'``), then
-
-      .. math::
-          \ell(x, y) = \begin{cases}
-              \frac{\sum_{n=1}^N l_n}{N}, &
-               \text{if reduction} = \text{`mean';}\\
-                \sum_{n=1}^N l_n,  &
-                \text{if reduction} = \text{`sum'.}
-            \end{cases}
-
-    .. note::
-        The performance of this criterion is generally better when `target` contains class
-        indices, as this allows for optimized computation. Consider providing `target` as
-        class probabilities only when a single class label per minibatch item is too restrictive.
 
     Args:
         weight (Tensor, optional): a manual rescaling weight given to each class.
-            If given, has to be a Tensor of size `C` and floating point dtype
+            If given, has to be a Tensor of size `C`
         size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
             the losses are averaged over each loss element in the batch. Note that for
             some losses, there are multiple elements per sample. If the field :attr:`size_average`
             is set to ``False``, the losses are instead summed for each minibatch. Ignored
-            when :attr:`reduce` is ``False``. Default: ``True``
+            when reduce is ``False``. Default: ``True``
         ignore_index (int, optional): Specifies a target value that is ignored
             and does not contribute to the input gradient. When :attr:`size_average` is
-            ``True``, the loss is averaged over non-ignored targets. Note that
-            :attr:`ignore_index` is only applicable when the target contains class indices.
+            ``True``, the loss is averaged over non-ignored targets.
         reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
             losses are averaged or summed over observations for each minibatch depending
             on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
             batch element instead and ignores :attr:`size_average`. Default: ``True``
-        reduction (str, optional): Specifies the reduction to apply to the output:
-            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will
-            be applied, ``'mean'``: the weighted mean of the output is taken,
-            ``'sum'``: the output will be summed. Note: :attr:`size_average`
-            and :attr:`reduce` are in the process of being deprecated, and in
-            the meantime, specifying either of those two args will override
-            :attr:`reduction`. Default: ``'mean'``
-        label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount
-            of smoothing when computing the loss, where 0.0 means no smoothing. The targets
-            become a mixture of the original ground truth and a uniform distribution as described in
-            `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.
+        reduction (string, optional): Specifies the reduction to apply to the output:
+            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
+            ``'mean'``: the sum of the output will be divided by the number of
+            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
+            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
+            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
 
     Shape:
-        - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
+        - Input: :math:`(N, C)` where `C = number of classes`, or
+          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
           in the case of `K`-dimensional loss.
-        - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with
-          :math:`K \geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.
-          If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.
-        - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
-          in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.
-
-
-        where:
-
-        .. math::
-            \begin{aligned}
-                C ={} & \text{number of classes} \\
-                N ={} & \text{batch size} \\
-            \end{aligned}
+        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or
+          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
+          K-dimensional loss.
+        - Output: scalar.
+          If :attr:`reduction` is ``'none'``, then the same size as the target:
+          :math:`(N)`, or
+          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
+          of K-dimensional loss.
 
     Examples::
 
-        >>> # Example of target with class indices
         >>> loss = nn.CrossEntropyLoss()
         >>> input = torch.randn(3, 5, requires_grad=True)
         >>> target = torch.empty(3, dtype=torch.long).random_(5)
         >>> output = loss(input, target)
         >>> output.backward()
-        >>>
-        >>> # Example of target with class probabilities
-        >>> input = torch.randn(3, 5, requires_grad=True)
-        >>> target = torch.randn(3, 5).softmax(dim=1)
-        >>> output = loss(input, target)
-        >>> output.backward()
     """
-    __constants__ = ["ignore_index", "reduction", "label_smoothing"]
-    ignore_index: int
-    label_smoothing: float
+    __constants__ = ['weight', 'ignore_index', 'reduction']
 
-    def __init__(
-        self,
-        weight: Optional[Tensor] = None,
-        size_average=None,
-        ignore_index: int = -100,
-        reduce=None,
-        reduction: str = "mean",
-        label_smoothing: float = 0.0,
-    ) -> None:
-        super().__init__(weight, size_average, reduce, reduction)
+    def __init__(self, weight=None, size_average=None, ignore_index=-100,
+                 reduce=None, reduction='mean'):
+        super(CrossEntropyLoss, self).__init__(weight, size_average, reduce, reduction)
         self.ignore_index = ignore_index
-        self.label_smoothing = label_smoothing
 
-    def forward(self, input: Tensor, target: Tensor) -> Tensor:
-        return F.cross_entropy(
-            input,
-            target,
-            weight=self.weight,
-            ignore_index=self.ignore_index,
-            reduction=self.reduction,
-            label_smoothing=self.label_smoothing,
-        )
+    def forward(self, input, target):
+        return F.cross_entropy(input, target, weight=self.weight,
+                               ignore_index=self.ignore_index, reduction=self.reduction)
 