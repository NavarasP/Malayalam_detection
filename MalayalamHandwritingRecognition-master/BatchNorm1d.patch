--- /opt/conda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py
+++ /opt/conda/envs/fastai/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py
@@ -1,9 +1,7 @@
 class BatchNorm1d(_BatchNorm):
-    r"""Applies Batch Normalization over a 2D or 3D input.
-
-    Method described in the paper
-    `Batch Normalization: Accelerating Deep Network Training by Reducing
-    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .
+    r"""Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
+    inputs with optional additional channel dimension) as described in the paper
+    `Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift`_ .
 
     .. math::
 
@@ -11,12 +9,8 @@
 
     The mean and standard-deviation are calculated per-dimension over
     the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
-    of size `C` (where `C` is the number of features or channels of the input). By default, the
-    elements of :math:`\gamma` are set to 1 and the elements of :math:`\beta` are set to 0.
-    At train time in the forward pass, the standard-deviation is calculated via the biased estimator,
-    equivalent to ``torch.var(input, unbiased=False)``. However, the value stored in the
-    moving average of the standard-deviation is calculated via the unbiased  estimator, equivalent to
-    ``torch.var(input, unbiased=True)``.
+    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
+    to 1 and the elements of :math:`\beta` are set to 0.
 
     Also by default, during training this layer keeps running estimates of its
     computed mean and variance, which are then used for normalization during
@@ -39,7 +33,8 @@
     on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.
 
     Args:
-        num_features: number of features or channels :math:`C` of the input
+        num_features: :math:`C` from an expected input of size
+            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`
         eps: a value added to the denominator for numerical stability.
             Default: 1e-5
         momentum: the value used for the running_mean and running_var
@@ -49,14 +44,11 @@
             learnable affine parameters. Default: ``True``
         track_running_stats: a boolean value that when set to ``True``, this
             module tracks the running mean and variance, and when set to ``False``,
-            this module does not track such statistics, and initializes statistics
-            buffers :attr:`running_mean` and :attr:`running_var` as ``None``.
-            When these buffers are ``None``, this module always uses batch statistics.
-            in both training and eval modes. Default: ``True``
+            this module does not track such statistics and always uses batch
+            statistics in both training and eval modes. Default: ``True``
 
     Shape:
-        - Input: :math:`(N, C)` or :math:`(N, C, L)`, where :math:`N` is the batch size,
-          :math:`C` is the number of features or channels, and :math:`L` is the sequence length
+        - Input: :math:`(N, C)` or :math:`(N, C, L)`
         - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)
 
     Examples::
@@ -67,9 +59,13 @@
         >>> m = nn.BatchNorm1d(100, affine=False)
         >>> input = torch.randn(20, 100)
         >>> output = m(input)
+
+    .. _`Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift`:
+        https://arxiv.org/abs/1502.03167
     """
 
     def _check_input_dim(self, input):
         if input.dim() != 2 and input.dim() != 3:
-            raise ValueError(f"expected 2D or 3D input (got {input.dim()}D input)")
+            raise ValueError('expected 2D or 3D input (got {}D input)'
+                             .format(input.dim()))
 